{
 "metadata": {
  "name": "nb_pandas-big"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analyzing PaperCut Log Files with [__`pandas`__](http://pandas.pydata.org/)\n",
      "===="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***\n",
      "Set up the [IPython](http://ipython.org/) Environment\n",
      "-----------------------------\n",
      "* Standard imports and aliases\n",
      "* Change some matplot lib defaults"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import re, sys, glob \n",
      "import pandas as pd\n",
      "from pandas import DataFrame, Series, Timestamp, DateOffset\n",
      "import matplotlib as mpl\n",
      "WIDTH = 16\n",
      "mpl.rcParams['axes.color_cycle'] = ['b', 'r', 'c', 'y', 'k', 'm']\n",
      "mpl.rcParams['legend.loc'] = 'best'\n",
      "mpl.rcParams['figure.figsize'] = [WIDTH, 4]\n",
      "np.set_printoptions(linewidth=200)\n",
      "pd.set_printoptions(max_rows=100, max_columns=10, precision=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "d:\\Anaconda\\lib\\site-packages\\pandas\\core\\format.py:1612: FutureWarning: set_printoptions is deprecated, use set_option instead\n",
        "  FutureWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the Library Versions\n",
      "-----\n",
      "The notebook has been tested with\n",
      "\n",
      "* [`Anaconda`](https://store.continuum.io/) `1.4.0` \n",
      "* [`pandas`](http://pandas.pydata.org/getpandas.html) `0.11.0`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'python:', sys.version\n",
      "print 'numpy:', np.__version__\n",
      "print 'matplotlib:', mpl.__version__\n",
      "print 'pandas:', pd.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "python: 2.7.4 |Anaconda 1.5.0 (64-bit)| (default, Apr  9 2013, 12:26:13) [MSC v.1500 64 bit (AMD64)]\n",
        "numpy: 1.7.1\n",
        "matplotlib: 1.2.1\n",
        "pandas: 0.11.0\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***\n",
      "Load the saved server logs \n",
      "--------------------------\n",
      "The logs were previously parsed and read into a pandas DataFrame.\n",
      "\n",
      "The DataFrame was saved in an HDF5 with the extension __`.h5`__. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!dir /o:d *.h5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Volume in drive D is SSD\n",
        " Volume Serial Number is 048D-9616\n",
        "\n",
        " Directory of D:\\peter.dev\\log_analysis\n",
        "\n",
        "06/05/2013  10:38 AM       116,710,088 pc_pandas.h5\n",
        "07/05/2013  10:41 AM       100,550,872 xxx.h5\n",
        "07/05/2013  01:50 PM        97,617,856 yyy.h5\n",
        "07/05/2013  04:38 PM         1,332,696 zzz.h5\n",
        "08/05/2013  02:31 PM        33,458,848 server_1_simple.h5\n",
        "09/05/2013  11:16 AM        24,379,616 can.simple.h5\n",
        "09/05/2013  11:37 AM        93,635,600 pc_can.simple.h5\n",
        "10/05/2013  10:13 AM        44,008,664 temp_p3DB92361.simple.h5\n",
        "10/05/2013  10:13 AM        33,458,848 test.h5\n",
        "20/05/2013  09:15 AM                 0 pc_can.h5\n",
        "              10 File(s)    545,153,088 bytes\n",
        "               0 Dir(s)  31,638,233,088 bytes free\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import HDFStore\n",
      "store = HDFStore('server_log_2.h5')\n",
      "store.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_hdf('server_log_2.h5', 'logs')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'No object named logs in the file'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-1624760aa9b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'server_log_2.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mget_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(store)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;34m\"\"\" read from the store, closeit if we opened it \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No object named %s in the file'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;31m# create the storer and axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'No object named logs in the file'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a look at the DataFrame. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "secs = (df.index.max() - df.index.min()).total_seconds()\n",
      "hours = secs/3600\n",
      "print '%1.f hours of logs' % hours"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '%d log entries/hour' % int(len(df)/hours)\n",
      "print '%.1f thousand log entries/hour' % (int(len(df)/hours)/1000.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The values of __`level`__ seen in the logs are given by ..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "levels = df.level.unique()\n",
      "levels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the numbers of entries with each __`level`__ are given by ..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for level in levels:\n",
      "    print '%-5s : %5d' % (level, len(df[df.level==level]))\n",
      "print 'Total : %d' % df.shape[0] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***\n",
      "Let's Plot Some Graphs\n",
      "-----------\n",
      "Let's look at the frequency of log messages over the duration of all the logs.\n",
      "\n",
      "This should give a rough indication of PaperCut usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime, timedelta\n",
      "\n",
      "def truncate_to_minutes(timestamp):\n",
      "    return Timestamp(datetime(timestamp.year, timestamp.month, timestamp.day, timestamp.hour, timestamp.minute))\n",
      "\n",
      "start_time, end_time = df.index.min(), df.index.max()\n",
      "start_time = truncate_to_minutes(start_time + timedelta(minutes=1))\n",
      "end_time = truncate_to_minutes(end_time - timedelta(minutes=-1))\n",
      "start_time, end_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2 = df.between_time(start_time, end_time)\n",
      "df2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minute_counts = df2.file.resample('1min', how='count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minute_counts.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "levels = df.level.unique()\n",
      "for level in levels:\n",
      "    bars = df[df.level==level].file.resample('10min', how='count')\n",
      "    bars.plot(label='%s : %d' % (level, len(df[df.level==level])))\n",
      "title('Log counts for 10 minute intervals from %s to %s' % (df.index.min(), df.index.max()))\n",
      "ylabel('Number of log entries per 10 minutes')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The `INFO` and `ERROR` level entries are hard to see at the above scale. Let's plot them by themselves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for level in ['INFO', 'ERROR']:\n",
      "    bars = df[df.level==level].file.resample('10min', how='count')\n",
      "    bars.plot(label='%s : %d' % (level, len(df[df.level==level])))\n",
      "title('Just INFO and ERROR counts')\n",
      "ylabel('Number of log entries per 10 minutes')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for level in ['ERROR']:\n",
      "    bars = df[df.level==level].file.resample('10min', how='count')\n",
      "    bars.plot(label='%s : %d' % (level, len(df[df.level==level])))\n",
      "title('Just ERROR counts')\n",
      "ylabel('Number of log entries per 10 minutes')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These graphs are very noisy which makes them hard to read.\n",
      "\n",
      "### Smooth the graphs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the unsmoothed data in log entries/hour\n",
      "unsmoothed = df.file.resample('1min', how='count', convention='center')* 60   \n",
      "#2012-09-28 09:16:37.073000 to 2012-09-27 23:02:26.175000\n",
      "unsmoothed['2012-09-28 00:00:00' : '2012-09-28 09:00:00'].plot(style='c', label='1 minute unsmoothed') \n",
      "\n",
      "# Plot the data sample at intervals of of 1, 10, and 60 minutes then smoothed to a 60 minute rolling sum.\n",
      "for period in 1, 20, 60:\n",
      "    rolling_sum = pd.rolling_sum(df.file.resample('%dmin' % period, how='count', convention='center'), 60//period, center=True )\n",
      "    rolling_sum['2012-09-28 00:00:00' : '2012-09-28 09:00:00'].plot(label='1 hour rolling sum %d parts' % (60//period)) \n",
      "\n",
      "hours = (df.index.max() - df.index.min()).total_seconds()/3600\n",
      "title(r'''Log counts per hour from %s to %s\n",
      "$Average=\\frac{%d}{%.1f}=%d$\n",
      "''' % (df.index.min(), df.index.max(), \n",
      "        len(df), hours, int(len(df)/hours)), size=16)\n",
      "ylabel('Number of log entries/hour')\n",
      "legend() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Plot the same data as a bar chart\n",
      "A bar chart is more common and some would say a more natural way to plot events"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Need to plot shorter bars over longer bars so we can see the shorter bars\n",
      "levels = sorted(levels, key = lambda x: -len(df[df.level==x]))\n",
      "# Color cycling is currenty broken on pandas bar chars so we implement our own\n",
      "colors = ['b', 'r', 'w']\n",
      "\n",
      "# Make the figure tall so we can see the tiny INFO and ERROR counts\n",
      "plt.figsize(WIDTH, 6)\n",
      "\n",
      "for i,level in enumerate(levels):\n",
      "    bars = df[df.level==level].file.resample('60min', how='count')\n",
      "    bars.plot(kind='bar', color=colors[i], label='%s : %d' % (level, len(df[df.level==level])))\n",
      "\n",
      "hours = (df.index.max() - df.index.min()).total_seconds()/3600\n",
      "title(r'''Log counts for 1 hour intervals from %s to %s\n",
      "$Average=\\frac{%d}{%.1f}=%d$\n",
      "''' % (df.index.min(), df.index.max(), \n",
      "        len(df), hours, int(len(df)/hours)), size=16)\n",
      "ylabel('Number of log entries/hour')\n",
      "legend() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a closer look at 1pm-2pm on 18th July\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_period(df, start, end, frequency_minutes):\n",
      "    df_period = df.between_time(start, end)\n",
      "    \n",
      "    for i,level in enumerate(levels):\n",
      "        bars = df_period[df_period.level==level].file.resample('%dmin' % frequency_minutes, how='count')\n",
      "        bars.plot(kind='bar', color=colors[i], label='%s : %d' % (level, len(df_period[df_period.level==level])))\n",
      "    \n",
      "    minutes = (df_period.index.max() - df_period.index.min()).total_seconds()/60\n",
      "    \n",
      "    title(r'''Log counts for %d minute intervals from %s to %s\n",
      "    $Average=\\frac{%d}{%.1f}=%d$\n",
      "    ''' % (frequency_minutes, df_period.index.min(), df_period.index.max(), \n",
      "           len(df_period), minutes, int(len(df_period)/minutes)), size=15)\n",
      "    ylabel('Number of log entries/minute') \n",
      "    legend()  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figsize(WIDTH, 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_period(df, '2012-09-28 03:00.00', '2012-09-28 03:59.59', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_period(df, '2012-09-28 04:00.00', '2012-09-28 04:59.59', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Source Files "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ufile = df.file.unique()\n",
      "print '%d source files' % len(ufile)\n",
      "print sorted(ufile[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***\n",
      "[GroupBy](http://pandas.pydata.org/pandas-docs/dev/groupby.html)\n",
      "-------\n",
      "An obvious way to group log entries is by `file:line`\n",
      "\n",
      "* `file` is the source file the message was logged from\n",
      "* `line` is the line of the log message in `file`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_line = df.groupby(['file', 'line'])\n",
      "file_line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Compute the number of logs enrties for each `file:line` combination\n",
      "* Sort them in descending order\n",
      "* Show the top 20"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_size = file_line.size()\n",
      "fl_sorted = fl_size.order(ascending=False)\n",
      "fl_sorted[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_percent = fl_sorted/fl_sorted.sum() * 100.0\n",
      "fl_percent[:20].plot(kind='barh')\n",
      "plt.xlabel('Percentage of log entries')\n",
      "plt.title('Spammiest Log Messages')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_percent_cum = fl_sorted.cumsum()/fl_sorted.sum() * 100\n",
      "fl_percent_cum[:20].plot(kind='barh')\n",
      "plt.xlabel('Cumulate percentage of log entries')\n",
      "plt.title('Cumulative Percentage of Spammiest Log Messages')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_sorted.index[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_sorted.index.values[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Filter out the 10 most frequent `file:line` combinations from the full PaperCut server log DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_df = df\n",
      "for fl, ln in fl_sorted.index.values[:10]:\n",
      "    clean_df = clean_df[(clean_df.line != ln) & (clean_df.file != fl)]\n",
      "print 'clean_df has %d entries of %d (%d%%)' % (len(clean_df), len(df), int(100*len(clean_df)/len(df)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_file_line = clean_df.groupby(['file', 'line'])\n",
      "clean_fl_size = clean_file_line.size()\n",
      "clean_fl_sorted = clean_fl_size.order(ascending=False)\n",
      "clean_fl_sorted[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_fl_percent = clean_fl_sorted/fl_sorted.sum() * 100.0\n",
      "clean_fl_percent[:20].plot(kind='barh')\n",
      "plt.xlabel('Percentage of log entries')\n",
      "plt.title('Next Spammiest Log Messages')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How correlated are the frequencies of the log messages?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fl, ln in fl_sorted.index.values[:10]:\n",
      "    entries = df[(df.file==fl) & (df.line==ln)]\n",
      "    rolling_sum = pd.rolling_sum(entries.resample('1min', how='count', convention='center'), 60, center=True)\n",
      "    rolling_sum = (rolling_sum-rolling_sum.mean())/rolling_sum.std()\n",
      "    rolling_sum['2012-09-28 00:00:00' : '2012-09-28 09:00:00'].plot(label='%s:%d' % (fl,ln))\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fl, ln in fl_sorted.index.values[5:15]:\n",
      "    entries = df[(df.file==fl) & (df.line==ln)]\n",
      "    rolling_sum = pd.rolling_sum(entries.resample('1min', how='count', convention='center'), 60, center=True)\n",
      "    rolling_sum = (rolling_sum-rolling_sum.mean())/rolling_sum.std()\n",
      "    rolling_sum['2012-09-28 00:00:00' : '2012-09-28 09:00:00'].plot(label='%s:%d' % (fl,ln))\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl_rolling_sums = []\n",
      "for fl,ln in fl_sorted.index.values[:10]:\n",
      "    entries = df[(df.file==fl) & (df.line==ln)]\n",
      "    rolling_sum = pd.rolling_sum(entries.resample('1min', how='count', convention='center'), 60, center=True)\n",
      "    rolling_sum = rolling_sum/rolling_sum.mean()\n",
      "    fl_rolling_sums.append(((fl,ln), rolling_sum))    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(linewidth=160)\n",
      "correlations = np.zeros((len(fl_rolling_sums),len(fl_rolling_sums)))\n",
      "for i,(_,rs1) in enumerate(fl_rolling_sums):\n",
      "    for j,(_,rs2) in enumerate(fl_rolling_sums):\n",
      "        correlations[i,j] = rs1.corr(rs2) #.corrwith(rs2)\n",
      "correlations        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(correlations) <= 0.8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = len(correlations)\n",
      "i_j_corr = [(i,j,correlations[i,j]) for i in range(N) for j in range(N) if i < j]\n",
      "i_j_corr.sort(key=lambda x: (x[2],x[0],x[1]))\n",
      "i_j_corr[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()\n",
      "pca.fit(correlations)\n",
      "print pca.explained_variance_ratio_\n",
      "print pca.explained_variance_ratio_.cumsum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* 69% of the variance in the data is explained by the first principal component\n",
      "* 92% of the variance in the data is explained by the first 2 principal components\n",
      "* 99% of the variance in the data is explained by the first 3 principal components"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The file:line breakdown of the top 3 principal components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "components = {\n",
      "      'file:line': ['%s:%d' % (fl,ln) for fl,ln in fl_sorted.index.values[:10]],\n",
      "}\n",
      "for i in range(3):\n",
      "    components['pc%d' % i] = pca.components_.T[i]\n",
      "comp_df = DataFrame(components)   \n",
      "comp_df['t01'] = comp_df.pc1 - comp_df.pc0\n",
      "comp_df['t02'] = comp_df.pc2 - comp_df.pc0\n",
      "comp_df['t12'] = comp_df.pc2 - comp_df.pc1\n",
      "pd.set_printoptions(max_rows=200, max_columns=20, precision=4)\n",
      "open('pca.html', 'wt').write(comp_df.to_html())\n",
      "comp_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pc0, pc1 and pc2 all have similar fractions of file:line 0-5. \n",
      "\n",
      "They are distinguished by\n",
      "\n",
      "* `HibernateDAOBase:91` __pc0__ differs from pc1 and pc2\n",
      "* `VCCConnection:2065` __pc1__ differs from pc0 and pc2\n",
      "* `ExtDeviceStateManagerImpl:684` __pc2__ differs from pc0 and pc1\n",
      "\n",
      "The 3 biggest principal components are similar. Their main diffference are in their fractions of `ExtDeviceStateManagerImpl:684`, `HibernateDAOBase:91` and `VCCConnection:2065` . These 3 log messages are mostly likely to be uncorrelated.\n",
      "\n",
      "The 3rd principal component is much smaller than the 2nd. Therefore\n",
      "\n",
      "* `ExtDeviceStateManagerImpl:684` should differ significantly from `HibernateDAOBase:91`\n",
      "* `VCCConnection:2065` should explain most of the remaining variation \n",
      "\n",
      "Lets look at their code to confirm this."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plot these 3 log messages' activity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_activity(df, label):\n",
      "    rolling_sum = pd.rolling_sum(df.file.resample('1min', how='count', convention='center'), 60, center=True)\n",
      "    #rolling_sum = (rolling_sum - rolling_sum.mean())/rolling_sum.std()\n",
      "    rolling_sum['2012-09-28 00:00:00' : '2012-09-28 09:00:00'].plot(label=label)\n",
      "for fl,ln in [('ExtDeviceStateManagerImpl', 684),\n",
      "              ('HibernateDAOBase', 91),\n",
      "              ('VCCConnection', 2065)]:\n",
      "    plot_activity(df[(df.file==fl) & (df.line==ln)], '%s:%d' % (fl,ln))\n",
      "ylabel('Log entries/hour')    \n",
      "legend()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Look at the source code. Does the graph above make sense?"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "svn info\n",
      "Path: .\n",
      "URL: https://secure.papercutsoftware.com/repos/projects/pc-ng/trunk\n",
      "Repository Root: https://secure.papercutsoftware.com/repos\n",
      "Repository UUID: 505ffd8f-26e6-0310-8e5c-c33a79a468e7\n",
      "Revision: 17368\n",
      "Node Kind: directory\n",
      "Schedule: normal\n",
      "Last Changed Author: priyanka\n",
      "Last Changed Rev: 17368\n",
      "Last Changed Date: 2012-05-10 12:03:29 +1000 (Thu, 10 May 2012)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "('ExtDeviceStateManagerImpl', 684)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "  /**\n",
      "     * Retrieves the status so it can be updated.  The status update time is also set to the current time.\n",
      "     * @param deviceId The device id.\n",
      "     * @return The device status.\n",
      "     */\n",
      "    private ExtDeviceStatus getStatusForUpdate(long deviceId) {\n",
      "        DeviceState state = getStateCreateIfRequired(deviceId, null);\n",
      "        \n",
      "        if (logger.isDebugEnabled()) {\n",
      "            logger.debug(\"Retrieve status for update. Last updated: \" + new Date(state.status.getStatusUpdateTime()));\n",
      "        }\n",
      "        \n",
      "        state.status.setStatusUpdateTime(System.currentTimeMillis());\n",
      "        return state.status;\n",
      "    }"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "('HibernateDAOBase', 91)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "/**\n",
      " * Provides the base for all Hibernate based DAOs.\n",
      " * \n",
      " * @author matt\n",
      " * \n",
      " * XXX: Consider implementing named query support instead of arbitrary queries to better externalise HQL from code.\n",
      " */\n",
      "@SuppressWarnings(\"unchecked\")\n",
      "public abstract class HibernateDAOBase extends HibernateDaoSupport implements DAO {\n",
      "    private static final Log logger = LogFactory.getLog(HibernateDAOBase.class);\n",
      "\n",
      "    /**\n",
      "     * @see biz.papercut.pcng.persistence.DAO#getObjectById(long)\n",
      "     */\n",
      "    public Domain getObjectById(long id) { \n",
      "        Domain result = (Domain) getHibernateTemplate().get(getPersistentClass(), Long.valueOf(id));\n",
      "        \n",
      "        if (logger.isDebugEnabled()) {\n",
      "            if (result == null) {\n",
      "                String shortName = StringUtils.removeStart(getPersistentClassName(), \"biz.papercut.pcng.domain.\");\n",
      "                logger.debug(\"No object of type '\" + shortName + \"' with id = \" + id);\n",
      "            } else {\n",
      "                if (s_debugLastFetchedId.get() != result.getId()) {\n",
      "                    logger.debug(\"Fetched: \" + result);\n",
      "                }\n",
      "                s_debugLastFetchedId.set(result.getId());\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        return result;\n",
      "    }\n",
      "    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      " ('VCCConnection', 2065)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "    /**\n",
      "     * Read the response from the device.\n",
      "     * @return The response.\n",
      "     * @throws IOException Raised if there was a problem reading the response.\n",
      "     */\n",
      "    private VCCResponse readResponse() throws IOException {\n",
      "        int responseLen = 0;\n",
      "        \n",
      "        ...\n",
      "         if (logger.isDebugEnabled()) {\n",
      "            logger.debug(\"RECV: \" + r.getResponseCodeDesc() \n",
      "                            + (r.getDataLength() > 0 ? \" Data: \" + r.getDataAsString() : \"\"));\n",
      "        }    \n",
      "        "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}