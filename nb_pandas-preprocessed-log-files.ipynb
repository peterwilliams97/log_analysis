{
 "metadata": {
  "name": "nb_pandas-preprocessed-log-files"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analyzing Preprocessed PaperCut Log File Data\n",
      "=============================================\n",
      "Analyze files created with load_logs.py and preprocess_logs.py\n",
      "\n",
      "The preprocessed files are:\n",
      "\n",
      "* __lfl_freq__ \\(`lfl_freq.h5`\\) Per-minute frequency counts of log entries over the time covered by the server logs for each __file:line__ \n",
      "* __lfl_freq_corr__ \\(`lfl_freq_corr.h5`\\) Correlations of each __file:line__ series in __lfl_freq__\n",
      "* __lfl_sorted__ \\(`lfl_sorted.pkl`\\) Lists of __\\(level,file,line\\)__ <==> __file:line__ string mappings "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up the [IPython](http://ipython.org/) Environment\n",
      "-----------------------------\n",
      "* Standard imports and aliases\n",
      "* Change some matplot lib defaults"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import re, sys, glob \n",
      "import pandas as pd\n",
      "from pandas import DataFrame, Series, Timestamp, DateOffset\n",
      "import matplotlib as mpl\n",
      "import os\n",
      "import cPickle as pickle\n",
      "from collections import OrderedDict\n",
      "from datetime import datetime, timedelta\n",
      "WIDTH = 20\n",
      "mpl.rcParams['axes.color_cycle'] = ['b', 'r', 'c', 'y', 'k', 'm']\n",
      "mpl.rcParams['legend.loc'] = 'best'\n",
      "mpl.rcParams['figure.figsize'] = [WIDTH, 4]\n",
      "np.set_printoptions(linewidth=200)\n",
      "pd.set_printoptions(max_rows=100, max_columns=10, precision=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the Library Versions\n",
      "-----\n",
      "The notebook has been tested with\n",
      "\n",
      "* [`Anaconda`](https://store.continuum.io/) `1.4.0` \n",
      "* [`pandas`](http://pandas.pydata.org/getpandas.html) `0.11.0`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'python:', sys.version\n",
      "print 'numpy:', np.__version__\n",
      "print 'matplotlib:', mpl.__version__\n",
      "print 'pandas:', pd.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "python: 2.7.3 |Anaconda 1.4.0 (64-bit)| (default, Feb 25 2013, 18:41:24) [MSC v.1500 64 bit (AMD64)]\n",
        "numpy: 1.7.0\n",
        "matplotlib: 1.2.0\n",
        "pandas: 0.11.0\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Find the saved data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!dir data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Volume in drive D is SSD\n",
        " Volume Serial Number is 048D-9616\n",
        "\n",
        " Directory of D:\\peter.dev\\log_analysis\\data\n",
        "\n",
        "14/05/2013  03:19 PM    <DIR>          .\n",
        "14/05/2013  03:19 PM    <DIR>          ..\n",
        "12/05/2013  10:03 PM    <DIR>          AAA\n",
        "13/05/2013  02:26 PM    <DIR>          AAA-Copy\n",
        "14/05/2013  11:29 AM    <DIR>          AAA-Copy-Copy\n",
        "14/05/2013  12:49 PM    <DIR>          ATW-259-68053-karlstad\n",
        "12/05/2013  10:04 PM    <DIR>          BIG\n",
        "12/05/2013  10:08 PM    <DIR>          CAN-581-15745\n",
        "14/05/2013  03:43 PM    <DIR>          Kingston\n",
        "               0 File(s)              0 bytes\n",
        "               9 Dir(s)  20,248,334,336 bytes free\n"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!dir data\\ATW-259-68053-karlstad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Volume in drive D is SSD\n",
        " Volume Serial Number is 048D-9616\n",
        "\n",
        " Directory of D:\\peter.dev\\log_analysis\\data\\ATW-259-68053-karlstad\n",
        "\n",
        "14/05/2013  12:49 PM    <DIR>          .\n",
        "14/05/2013  12:49 PM    <DIR>          ..\n",
        "14/05/2013  12:41 PM               567 details.pkl\n",
        "14/05/2013  12:41 PM            20,259 entry_types.pkl\n",
        "14/05/2013  12:19 PM            38,926 history.pkl\n",
        "14/05/2013  12:49 PM         1,126,600 lfl_freq.h5\n",
        "14/05/2013  12:49 PM           472,796 lfl_freq_corr.h5\n",
        "14/05/2013  12:41 PM            26,747 lfl_sorted.pkl\n",
        "14/05/2013  12:19 PM       220,340,152 logs.h5\n",
        "14/05/2013  11:51 AM    <DIR>          temp\n",
        "               7 File(s)    222,026,047 bytes\n",
        "               3 Dir(s)  20,248,334,336 bytes free\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load the preprocessed data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data_dir = r'data\\AAA'\n",
      "#data_dir = r'data\\AAA-Copy'\n",
      "#data_dir = r'data\\AAA-Copy-Copy'\n",
      "#data_dir = r'data\\BIG'\n",
      "#data_dir = r'data\\CAN-581-15745_2'\n",
      "data_dir = r'data\\ATW-259-68053-karlstad'\n",
      "#data_dir = r'data\\Kingston'\n",
      "lfl_freq = pd.read_hdf(os.path.join(data_dir, 'lfl_freq.h5'), 'logs')\n",
      "lfl_freq_corr = pd.read_hdf(os.path.join(data_dir, 'lfl_freq_corr.h5'), 'logs')\n",
      "lfl_sorted = pickle.load(open(os.path.join(data_dir, 'lfl_sorted.pkl'), 'rb'))\n",
      "history = pickle.load(open(os.path.join(data_dir, 'history.pkl'), 'rb'))\n",
      "details = pickle.load(open(os.path.join(data_dir, 'details.pkl'), 'rb'))\n",
      "# Choose a server.log header at random from all the saved headers\n",
      "header = history[history.keys()[0]]['header']\n",
      "case_name = data_dir.split('\\\\')[-1]\n",
      "case_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "``data\\CAN-581-15745_2`` does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-114-5db62849a469>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#data_dir = r'data\\ATW-259-68053-karlstad'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#data_dir = r'data\\Kingston'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlfl_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lfl_freq.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mlfl_freq_corr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lfl_freq_corr.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mlfl_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lfl_sorted.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mget_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mget_store\u001b[1;34m(path, mode, complevel, complib, fletcher32)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         store = HDFStore(path, mode=mode, complevel=complevel,\n\u001b[1;32m--> 143\u001b[1;33m                          complib=complib, fletcher32=False)\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, warn)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'can not be written'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.pyc\u001b[0m in \u001b[0;36mh5_open\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mh5_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\tables\\file.pyc\u001b[0m in \u001b[0;36mopenFile\u001b[1;34m(filename, mode, title, rootUEP, filters, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrootUEP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\tables\\file.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, title, rootUEP, filters, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\tables\\hdf5Extension.pyd\u001b[0m in \u001b[0;36mtables.hdf5Extension.File._g_new (tables\\hdf5Extension.c:2690)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\tables\\utils.pyc\u001b[0m in \u001b[0;36mcheckFileAccess\u001b[1;34m(filename, mode)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcheckFileAccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mcheckFileAccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mcheckFileAccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32md:\\Anaconda3\\lib\\site-packages\\tables\\utils.pyc\u001b[0m in \u001b[0;36mcheckFileAccess\u001b[1;34m(filename, mode)\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[0mparentname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` does not exist\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparentname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparentname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` is not a directory\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparentname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: ``data\\CAN-581-15745_2`` does not exist"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Does the data look right?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "details"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lfl_freq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Are the numbers of entries roughly consistent? This numbers should be less than but close to 1.0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lfl_freq.sum(axis=1).sum()/details['count']['level']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculate some useful things"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file:line uniquely identifies each level,file,line\n",
      "# Construct mappings in both directions\n",
      "lfl_to_string = OrderedDict(((lvl,fl,ln), '%s:%d' % (fl,ln)) for lvl,fl,ln in lfl_sorted.index)\n",
      "string_to_lfl = OrderedDict(('%s:%d' % (fl,ln), (lvl,fl,ln)) for lvl,fl,ln in lfl_sorted.index)\n",
      "keys = string_to_lfl.keys()\n",
      "levels = set(lvl for lvl,fl,ln in lfl_sorted.index)\n",
      "keys_by_level = {level: [lfl_to_string[(lvl,fl,ln)] for lvl,fl,ln in lfl_sorted.index if lvl==level]\n",
      "                 for level in levels}\n",
      "levels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in string_to_lfl.items()[:5]: \n",
      "    print '%s: %s, %d' % (k, v, int(lfl_freq[k].sum()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's Plot Some Graphs\n",
      "-----------\n",
      "Let's look at the frequency of log messages over the duration of all the logs.\n",
      "\n",
      "This should give a rough indication of PaperCut usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def embellish(df):\n",
      "    \"\"\"Add standard title and axis labels to a graph\"\"\"\n",
      "    title('%s: Log entries/minute from %s to %s' % (case_name, df.index.min(), df.index.max()), fontweight='bold')\n",
      "    xlabel('Date/Time')\n",
      "    ylabel('Log entries/minute')\n",
      "    legend()\n",
      "    \n",
      "def get_peak(series):\n",
      "    return series.dropna().idxmax()\n",
      "\n",
      "def annotate_peak(series):\n",
      "    peak = get_peak(series)\n",
      "    plt.annotate(\n",
      "        '%d entries/minute at %s' % (series[peak], peak), \n",
      "        xy = (peak, totals[peak]), \n",
      "        xytext = (-20, -20), textcoords='offset points', \n",
      "        ha = 'right', va = 'bottom', bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
      "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "\n",
      "totals = lfl_freq.sum(axis=1)\n",
      "peak = get_peak(totals)\n",
      "\n",
      "totals.plot(label='Total counts : %6d' % (totals.sum()))\n",
      "embellish(lfl_freq)\n",
      "annotate_peak(totals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PaperCut usage sees to be high from 9am to 3pm then taper off to 6pm and is not used at all from 9pm to 5am. That seems reasonable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The `INFO` and `ERROR` level entries are hard to see at the above scale. Let's plot them by themselves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_levels(df, levels):\n",
      "    for lvl in levels:\n",
      "        if lvl not in keys_by_level:\n",
      "            continue\n",
      "        keys = keys_by_level[lvl]\n",
      "        parts = DataFrame({k: df[k] for k in keys}, columns=keys)\n",
      "        totals = parts.sum(axis=1).dropna()\n",
      "        totals.plot(label='%s : %6d' % (lvl, int(totals.sum())))\n",
      "        #annotate_peak(totals)\n",
      "    embellish(df)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_levels(lfl_freq, ['DEBUG', 'INFO', 'ERROR'])   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_levels(lfl_freq, ['INFO', 'ERROR'])   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_levels(lfl_freq, ['ERROR'])   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a closer look at the peak\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "totals = lfl_freq.sum(axis=1)\n",
      "peak = get_peak(totals)\n",
      "start = max(peak - timedelta(minutes=60), totals.index.min())\n",
      "end = min(start + timedelta(minutes=120), totals.index.max())\n",
      "peak_freqs = lfl_freq[start : end]\n",
      "plot_levels(peak_freqs, ['DEBUG', 'INFO', 'ERROR'])  \n",
      "annotate_peak(totals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = Series({k: lfl_freq[k].sum() for k in keys}, index=keys) \n",
      "counts[:20].plot(kind='barh')\n",
      "plt.xlabel('Percentage of log entries')\n",
      "plt.title('Spammiest Log Messages')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cum_counts = counts.cumsum()\n",
      "cum_pc = cum_counts/cum_counts.max() * 100.0\n",
      "plt.plot(np.arange(cum_pc.shape[0]), cum_pc)\n",
      "plt.ylabel('Cumulative percentage of log entries')\n",
      "plt.xlabel('Number of different log entry types')\n",
      "plt.title('Cumulative Percentage of Log Entries vs. Number of Entry Types', fontweight='bold')\n",
      "plt.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How correlated are the frequencies of the log messages?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def standardize(series):\n",
      "    \"\"\"Standardize serieso to mean 0 and stdev 1\"\"\"\n",
      "    ret = (series - series.mean())/series.std()\n",
      "    assert abs(ret.mean()) < 1e-6 \n",
      "    assert abs(ret.std()) - 1.0 < 1e-6\n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in keys[:5]:\n",
      "    standardize(lfl_freq[k]).plot()\n",
      "embellish(lfl_freq)    \n",
      "legend(loc='lower left', bbox_to_anchor=(0., 1.1), ncol=5, fancybox=True, shadow=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(linewidth=160)\n",
      "lfl_freq_corr.values[:10,:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.abs(lfl_freq_corr.values[:10,:10]) <= 0.7      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The uncorrelated log messages\n",
      "CORR_THRESH = 0.3\n",
      "\n",
      "uncorrelated = OrderedDict()\n",
      "uncorrelated[keys[0]] =  0.0\n",
      "for k in keys[1:]:\n",
      "    corr = max(lfl_freq_corr[k][i] for i in uncorrelated.keys())\n",
      "    if corr < CORR_THRESH:\n",
      "        uncorrelated[k] = corr\n",
      "Series(uncorrelated.values(), index=uncorrelated.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in uncorrelated.keys()[:5]:\n",
      "    standardize(lfl_freq[k]).plot()\n",
      "embellish(lfl_freq)    \n",
      "legend(loc='lower left', bbox_to_anchor=(0., 1.1), ncol=5, fancybox=True, shadow=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()\n",
      "# Take a slice of a noisy part of the graph. \n",
      "# This will depend on the data!\n",
      "# 2012-07-18 08:16:00 to 2012-07-19 09:22:00\n",
      "pca.fit(standardize(lfl_freq['2012-09-27 14:15:00' : '2012-09-27 15:15:00'].replace(nan,0.0).values))\n",
      "#pca.fit(standardize(lfl_freq.replace(nan,0.0).values))\n",
      "print pca.explained_variance_ratio_[:10]\n",
      "print pca.explained_variance_ratio_.cumsum()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The file:line breakdown of the top 3 principal components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "components = {'keys': keys}\n",
      "for i in range(3):\n",
      "    components['pc%d' % i] = pca.components_.T[i]\n",
      "comp_df = DataFrame(components)   \n",
      "comp_df['t01'] = comp_df.pc1 - comp_df.pc0\n",
      "comp_df['t02'] = comp_df.pc2 - comp_df.pc0\n",
      "comp_df['t12'] = comp_df.pc2 - comp_df.pc1\n",
      "pd.set_printoptions(max_rows=200, max_columns=20, precision=4)\n",
      "comp_df[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}